{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstMatRow = 0\n",
    "firstMatCol = 0\n",
    "secondMatRow = 0\n",
    "secondMatCol = 0\n",
    "resultMatRow = 0\n",
    "resultMatCol = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper1\n",
    "Split the original file by \",\" so it transform to a easier rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(line):\n",
    "    matrix, row, cal, val = line.split(\",\")\n",
    "    maplist = []\n",
    "    maplist.append((matrix, row, cal, val))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper2\n",
    "Sort the rdd lines according to which matrix it belongs to\n",
    "Let's say the value in the matrix is Mij, then for the first matrix we set the key-value pair (j, (M, i, Mij))\n",
    "For the second matrix Njk, we set the key-value pair (j, (N, k, Njk))\n",
    "By this, we can multiply them easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper2(line):\n",
    "    maplist = []\n",
    "    if line[0] == 'M':\n",
    "        maplist.append((int(line[2]), (line[0], int(line[1]), int(line[3]))))\n",
    "    elif line[0] == 'N':\n",
    "        maplist.append((int(line[1]), (line[0], int(line[2]), int(line[3]))))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper3\n",
    "By multiplying each element with the same key, we can get the key-value pair((i, k), Mij*Njk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper3(line):\n",
    "    maplist = []\n",
    "    maplist.append(((line[1][0][1], line[1][1][1]), line[1][0][2]*line[1][1][2]))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer1\n",
    "Add the values with the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer1(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "I searched many related articles on the internet for mapreduce in matrix multiplication,\n",
    "most of the ways they provide is to map the keys to the result matrix, and put the M's and N's values to that key with same j,\n",
    "after this, we simply multiply those values with the same js (but in different matrix),\n",
    "then we add up those values to get the final result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MatrixMultiplication\")\n",
    "sc = SparkContext(conf=conf)\n",
    "inputfile = sc.textFile(\"500input.txt\").flatMap(mapper1)\n",
    "\n",
    "# find matrix row and col\n",
    "matrixM = inputfile.filter(lambda x: \"M\" in x[0])\n",
    "matrixN = inputfile.filter(lambda x: \"N\" in x[0])\n",
    "firstMatRow = matrixM.max(lambda x: x[1])[1]\n",
    "firstMatCol = matrixM.max(lambda x: x[2])[2]\n",
    "secondMatRow = matrixN.max(lambda x: x[1])[1]\n",
    "secondMatCol = matrixN.max(lambda x: x[2])[2]\n",
    "resultMatRow = int(firstMatRow) + 1\n",
    "resultMatCol = int(secondMatCol) + 1\n",
    "\n",
    "# mapping1\n",
    "mappedfileM = matrixM.flatMap(mapper2)\n",
    "mappedfileN = matrixN.flatMap(mapper2)\n",
    "\n",
    "# mapping2\n",
    "newItems = mappedfileM.join(mappedfileN)\n",
    "#print(newItems.collect())\n",
    "reducedfile = newItems.flatMap(mapper3)\n",
    "#print(reducedfile.collect())\n",
    "finalReduce = reducedfile.reduceByKey(reducer1)\n",
    "#print(finalReduce.collect())\n",
    "#print(finalReduce.sortByKey().collect())\n",
    "sortedReduce = finalReduce.sortByKey()\n",
    "    \n",
    "outputFile = open(\"Outputfile.txt\",\"w\")\n",
    "for key, value in sortedReduce.collect():\n",
    "    result = str(key[0]) + \",\" + str(key[1]) + \",\" + str(value)\n",
    "    outputFile.write(\"%s\" % (result) + \"\\n\")\n",
    "outputFile.close()\n",
    "\n",
    "sc.stop()\n",
    "#print(inputfile.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
