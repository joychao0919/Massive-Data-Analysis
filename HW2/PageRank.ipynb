{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapper1\n",
    "Split the original file by ASCII 9 (long space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(line):\n",
    "    fromNode, toNode = line.split(chr(9))\n",
    "    maplist = []\n",
    "    maplist.append((fromNode, toNode))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reducer1\n",
    "while initialize the rankings, merge the same node without changing their rankings 1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer1(x, y):\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reducer2\n",
    "add all contributions for each nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer2(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapper3\n",
    "calculate the contribution, since I add self-to-self route, I separate it from the original ones, for self-to-self the contribution is 0, for original ones the contribution is rank/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper3(self, urls, rank):\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        if self == url:\n",
    "            yield (url, 0)\n",
    "        else:\n",
    "            yield (url, rank / (num_urls-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapper4\n",
    "renormalize the map by adding S, S is calculated according to the formula on the HW2 pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper4(line1, line2, S):\n",
    "    maplist = []\n",
    "    maplist.append((line1, line2 + S))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapper5\n",
    "to split the original data in to in-node and out-node, this mapper is for in-node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper5(line):\n",
    "    maplist = []\n",
    "    maplist.append(line[0])\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapper5\n",
    "to split the original data in to in-node and out-node, this mapper is for out-node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper6(line):\n",
    "    maplist = []\n",
    "    maplist.append(line[1])\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056\t0.000632\n",
      "1054\t0.000629\n",
      "1536\t0.000524\n",
      "171\t0.000512\n",
      "453\t0.000496\n",
      "407\t0.000485\n",
      "263\t0.000480\n",
      "4664\t0.000470\n",
      "261\t0.000463\n",
      "410\t0.000462\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"wordcount\")\n",
    "sc = SparkContext(conf=conf)\n",
    "links = sc.textFile(\"data2.txt\").flatMap(mapper1)\n",
    "\n",
    "urlNum1 = links.flatMap(mapper5)\n",
    "urlNum2 = links.flatMap(mapper6)\n",
    "urlNum = urlNum1.union(urlNum2).distinct().count()\n",
    "#print(urlNum)\n",
    "\n",
    "ranks1 = links.map(lambda line : (line[0], 1/int(urlNum))).reduceByKey(reducer1)\n",
    "ranks2 = links.map(lambda line : (line[1], 1/int(urlNum))).reduceByKey(reducer1)\n",
    "ranks = ranks1.union(ranks2).distinct().sortByKey()\n",
    "#print(ranks.collect())\n",
    "\n",
    "subnode_links = ranks.map(lambda line: (line[0], line[0]))\n",
    "join_links = links.union(subnode_links).distinct().sortByKey()\n",
    "#print(subnode_links.collect())\n",
    "\n",
    "lines = join_links.distinct().groupByKey().mapValues(lambda x: list(x)).cache()\n",
    "contribs = lines.join(ranks)\n",
    "#print(contribs.collect())\n",
    "\n",
    "for i in range(20):\n",
    "    contribs = lines.join(ranks).flatMap(lambda x: mapper3(x[0], x[1][0], x[1][1]))\n",
    "    #print(contribs.collect())\n",
    "    ranks = contribs.reduceByKey(reducer2).mapValues(lambda rank: rank*0.8 + 0.2/int(urlNum))\n",
    "    #print(ranks.collect())\n",
    "    sigma = ranks.map(lambda x : x[1]).sum()\n",
    "    S = (1 - sigma)/int(urlNum)\n",
    "    #print(S)\n",
    "    ranks = ranks.flatMap(lambda x: mapper4(x[0], x[1], S)).sortBy(lambda a: -a[1])\n",
    "    #print(newranks.collect())\n",
    "\n",
    "count = 0\n",
    "for (link, rank) in ranks.collect():\n",
    "    if(count < 10):\n",
    "        #print(\"%s\t%s\" % (link, rank))\n",
    "        print(\"%s\t%.06f\" % (link, rank))\n",
    "        count += 1\n",
    "    elif(count == 10):\n",
    "        break\n",
    "\n",
    "'''\n",
    "outputFile = open(\"Outputfile.txt\", \"w\")\n",
    "for (link, rank) in ranks.collect():\n",
    "    result = str(link) + chr(9) + str(format(rank,'.03f'))\n",
    "    outputFile.write(\"%s\" % (result) + \"\\n\")\n",
    "    #print(\"%s has rank: %.03f\" % (link, rank))\n",
    "outputFile.close()\n",
    "'''\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
